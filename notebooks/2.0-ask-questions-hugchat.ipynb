{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from hugchat import hugchat\n",
    "from hugchat.login import Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv(\"./secrets.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = os.environ['HUGGING_CHAT_USERNAME']\n",
    "PASSWD = os.environ['HUGGING_CHAT_PASSWORD']\n",
    "cookie_path_dir = \"./cookies/\" # NOTE: trailing slash (/) is required to avoid errors\n",
    "sign = Login(EMAIL, PASSWD)\n",
    "cookies = sign.login(cookie_dir_path=cookie_path_dir, save_cookies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Centauri! Our closest neighboring star system, located about 4.37 light-years from Earth. Here's a rundown of this fascinating system:\n",
      "\n",
      "**The System:**\n",
      "Alpha Centauri is a triple star system, consisting of three stars: Alpha Centauri A, Alpha Centauri B, and Proxima Centauri. The system is named after the brightest star, Alpha Centauri A.\n",
      "\n",
      "**The Stars:**\n",
      "\n",
      "1. **Alpha Centauri A (α Cen A)**: A G-type main-sequence star (similar to the Sun), with a mass about 1.1 times that of the Sun and a surface temperature of about 5,800 K (5,500°C or 10,000°F).\n",
      "2. **Alpha Centauri B (α Cen B)**: A K-type main-sequence star, with a mass about 0.9 times that of the Sun and a surface temperature of about 5,200 K (5,000°C or 9,000°F).\n",
      "3. **Proxima Centauri (α Cen C)**: A small, cool M-type red dwarf star, with a mass about 0.12 times that of the Sun and a surface temperature of about 3,000 K (2,700°C or 5,000°F). Proxima Centauri is the closest star to the Sun and is thought to be gravitationally bound to the Alpha Centauri system.\n",
      "\n",
      "**Orbital Characteristics:**\n",
      "The three stars in the Alpha Centauri system are in a hierarchical orbit, with Alpha Centauri A and B forming a close binary pair, and Proxima Centauri orbiting the pair at a much greater distance.\n",
      "\n",
      "* Alpha Centauri A and B orbit each other every 79.91 years, with an average distance of about 23.7 astronomical units (AU, where 1 AU is the average distance between the Earth and the Sun).\n",
      "* Proxima Centauri orbits the Alpha Centauri AB pair every 547,000 years, with an average distance of about 13,000 AU.\n",
      "\n",
      "**Planetary System:**\n",
      "In 2016, the discovery of a planet, Proxima b, was announced orbiting Proxima Centauri. Proxima b is a potentially habitable exoplanet, with a mass similar to that of Earth and an orbit within the habitable zone of its star, where liquid water could exist on its surface.\n",
      "\n",
      "**Exploration and Interest:**\n",
      "Alpha Centauri has been a target for scientific study and exploration due to its proximity to Earth and potential for hosting planets. Several missions have been proposed or are underway to explore the system, including:\n",
      "\n",
      "* Breakthrough Starshot: Aims to develop a laser-powered light sail that could accelerate a spacecraft to 20% of the speed of light, potentially reaching Alpha Centauri in just over 20 years.\n",
      "* Project Longshot: A proposed mission to send a robotic spacecraft to Alpha Centauri, with a planned launch in the 2060s.\n",
      "* The 100 Year Starship Project: A long-term initiative to develop a human settlement on a planet in the Alpha Centauri system.\n",
      "\n",
      "The Alpha Centauri system offers a fascinating opportunity for scientists to study the properties of nearby stars, search for exoplanets, and potentially even establish a human presence in the near future.\n"
     ]
    }
   ],
   "source": [
    "chatbot = hugchat.ChatBot(cookies=cookies.get_dict())\n",
    "\n",
    "print(chatbot.chat(\"Tell me about Alpha Centauri\").wait_until_done())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta-llama/Llama-3.3-70B-Instruct',\n",
       " 'Qwen/Qwen2.5-72B-Instruct',\n",
       " 'CohereForAI/c4ai-command-r-plus-08-2024',\n",
       " 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B',\n",
       " 'nvidia/Llama-3.1-Nemotron-70B-Instruct-HF',\n",
       " 'Qwen/QwQ-32B-Preview',\n",
       " 'Qwen/Qwen2.5-Coder-32B-Instruct',\n",
       " 'meta-llama/Llama-3.2-11B-Vision-Instruct',\n",
       " 'NousResearch/Hermes-3-Llama-3.1-8B',\n",
       " 'mistralai/Mistral-Nemo-Instruct-2407',\n",
       " 'microsoft/Phi-3.5-mini-instruct']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = chatbot.get_available_llm_models()\n",
    "display([m.name for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_model_name = 'NousResearch/Hermes-3-Llama-3.1-8B'\n",
    "model_names = [m.name for m in models]\n",
    "idx = model_names.index(chosen_model_name)\n",
    "\n",
    "chatbot.switch_llm(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Centauri! Our closest neighboring star system, located about 4.37 light-years from Earth. Here's a comprehensive overview:\n",
      "\n",
      "**Introduction**\n",
      "Alpha Centauri is a triple star system, consisting of three stars: Alpha Centauri A, Alpha Centauri B, and Proxima Centauri. The system is named after the brightest star, Alpha Centauri A. With a distance of just 4.37 light-years, Alpha Centauri is the closest star system to our Sun, making it an exciting target for astronomers and space enthusiasts alike.\n",
      "\n",
      "**The Stars**\n",
      "\n",
      "1. **Alpha Centauri A (α Cen A)**: A G-type main-sequence star (similar to the Sun), with a mass about 1.1 times that of the Sun and a surface temperature of about 5,800 K (5,500°C or 10,000°F). Alpha Centauri A is the brightest and most massive star in the system.\n",
      "2. **Alpha Centauri B (α Cen B)**: A K-type main-sequence star, with a mass about 0.9 times that of the Sun and a surface temperature of about 5,200 K (5,000°C or 9,000°F). Alpha Centauri B is slightly smaller and cooler than Alpha Centauri A.\n",
      "3. **Proxima Centauri (α Cen C)**: A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/anurags/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/hugchat/hugchat.py\", line 724, in _stream_query\n",
      "    for line in resp.iter_lines(decode_unicode=True):\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anurags/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/requests/models.py\", line 869, in iter_lines\n",
      "    for chunk in self.iter_content(\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anurags/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/requests/utils.py\", line 572, in stream_decode_response_unicode\n",
      "    for chunk in iterator:\n",
      "                 ^^^^^^^^\n",
      "  File \"/Users/anurags/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/requests/models.py\", line 820, in generate\n",
      "    yield from self.raw.stream(chunk_size, decode_content=True)\n",
      "  File \"/Users/anurags/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/urllib3/response.py\", line 1063, in stream\n",
      "    yield from self.read_chunked(amt, decode_content=decode_content)\n",
      "  File \"/Users/anurags/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/urllib3/response.py\", line 1219, in read_chunked\n",
      "    self._update_chunk_length()\n",
      "  File \"/Users/anurags/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/urllib3/response.py\", line 1138, in _update_chunk_length\n",
      "    line = self._fp.fp.readline()  # type: ignore[union-attr]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anurags/.pyenv/versions/3.12.9/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anurags/.pyenv/versions/3.12.9/lib/python3.12/ssl.py\", line 1251, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anurags/.pyenv/versions/3.12.9/lib/python3.12/ssl.py\", line 1103, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "ChatError",
     "evalue": "Failed to parse response: {\"type\":\"stream\",\"token\":\" A\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/hugchat/hugchat.py:724\u001b[39m, in \u001b[36mChatBot._stream_query\u001b[39m\u001b[34m(self, text, web_search, is_retry, retry_count, conversation, message_id)\u001b[39m\n\u001b[32m    723\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/requests/models.py:869\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self, chunk_size, decode_unicode, delimiter)\u001b[39m\n\u001b[32m    867\u001b[39m pending = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/requests/utils.py:572\u001b[39m, in \u001b[36mstream_decode_response_unicode\u001b[39m\u001b[34m(iterator, r)\u001b[39m\n\u001b[32m    571\u001b[39m decoder = codecs.getincrementaldecoder(r.encoding)(errors=\u001b[33m\"\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/urllib3/response.py:1063\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/urllib3/response.py:1219\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1218\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/urllib3/response.py:1138\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1139\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mChatError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# print(chatbot.chat(\"Tell me about Alpha Centauri\").wait_until_done())\u001b[39;00m\n\u001b[32m      2\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mTell me about Alpha Centauri\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchatbot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTell me about Alpha Centauri\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtoken\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/hugchat/message.py:201\u001b[39m, in \u001b[36mMessage.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28mself\u001b[39m.error = e\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.msg_status = MessageStatus.REJECTED\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/hugchat/message.py:123\u001b[39m, in \u001b[36mMessage.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    120\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMessage status is `Rejected` but no error found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     data: \u001b[38;5;28mdict\u001b[39m = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m._filterResponse(data)\n\u001b[32m    125\u001b[39m     data_type: \u001b[38;5;28mstr\u001b[39m = data[\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/book-rag-o4xYDdTl-py3.12/lib/python3.12/site-packages/hugchat/hugchat.py:760\u001b[39m, in \u001b[36mChatBot._stream_query\u001b[39m\u001b[34m(self, text, web_search, is_retry, retry_count, conversation, message_id)\u001b[39m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mConversation not found\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(res):\n\u001b[32m    759\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.InvalidConversationIDError(\u001b[33m\"\u001b[39m\u001b[33mConversation id invalid\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.ChatError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to parse response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m break_flag:\n\u001b[32m    762\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mChatError\u001b[39m: Failed to parse response: {\"type\":\"stream\",\"token\":\" A\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\"}"
     ]
    }
   ],
   "source": [
    "# print(chatbot.chat(\"Tell me about Alpha Centauri\").wait_until_done())\n",
    "prompt = \"Tell me about Alpha Centauri\"\n",
    "\n",
    "for chunk in chatbot.chat(\"Tell me about Alpha Centauri\"):\n",
    "    print(chunk[\"token\"], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ''\n",
    "with open(os.path.join('../data', 'attention-is-all-you-need.txt'), 'r') as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 interesting questions one could ask about the Transformer paper:\n",
      "\n",
      "1. **How does the self-attention mechanism in the Transformer model capture long-range dependencies in input sequences, and what are the implications of this for tasks like machine translation?**\n",
      "\n",
      "This question gets at the heart of the Transformer's innovative architecture and its ability to handle complex dependencies in input data.\n",
      "\n",
      "2. **What are the trade-offs between using a large number of attention heads versus a small number of attention heads with larger key and value dimensions, and how do these trade-offs impact model performance?**\n",
      "\n",
      "This question encourages discussion of the hyperparameter choices made in the paper and how they affect the model's behavior.\n",
      "\n",
      "3. **How does the use of sinusoidal positional encodings in the Transformer model compare to other methods for incorporating positional information, such as learned positional embeddings?**\n",
      "\n",
      "This question invites comparison of different approaches to encoding positional information and their relative merits.\n",
      "\n",
      "4. **In what ways does the Transformer model's parallelization of self-attention computations contribute to its speed and efficiency, and what are the implications of this for large-scale sequence transduction tasks?**\n",
      "\n",
      "This question highlights the Transformer's computational advantages and their impact on real-world applications.\n",
      "\n",
      "5. **How does the Transformer model's performance on English-to-German and English-to-French translation tasks compare to other state-of-the-art models, and what do these results suggest about the model's strengths and weaknesses?**\n",
      "\n",
      "This question encourages analysis of the paper's experimental results and their significance for the field of machine translation.\n",
      "\n",
      "6. **What are the potential benefits and drawbacks of using a single, shared weight matrix for the input and output embeddings in the Transformer model, and how might this design choice be modified or extended in future work?**\n",
      "\n",
      "This question prompts discussion of the model's embedding architecture and its potential limitations or opportunities for improvement.\n",
      "\n",
      "7. **In what ways does the Transformer model's use of layer normalization and residual connections contribute to its stability and effectiveness, and how do these components interact with the self-attention mechanism?**\n",
      "\n",
      "This question delves into the details of the Transformer's architecture and how its various components work together.\n",
      "\n",
      "8. **How might the Transformer model be adapted or extended to handle sequence transduction tasks beyond machine translation, such as text summarization or question answering, and what new challenges or opportunities might arise in these contexts?**\n",
      "\n",
      "This question invites speculation about the Transformer's potential applications and the challenges of applying it to new domains.\n",
      "\n",
      "9. **What are the implications of the Transformer model's performance on English constituency parsing for our understanding of the relationship between sequence transduction and syntactic parsing, and how might this work inform future research in these areas?**\n",
      "\n",
      "This question encourages analysis of the paper's results on constituency parsing and their broader significance for the field of natural language processing.\n",
      "\n",
      "10. **How does the Transformer model's attention visualization provide insights into the model's decision-making process, and what do these visualizations reveal about the model's strengths and weaknesses in handling different types of input sequences?**\n",
      "\n",
      "This question highlights the value of attention visualization as a tool for understanding the Transformer's behavior and invites discussion of the insights that can be gained from these visualizations."
     ]
    }
   ],
   "source": [
    "prompt = \"Read the following paper, don't summarize, and you must share 10 interesting questions one could ask on this paper.\\n{}\\n Don't forget to share 10 interesting questions one could ask on this paper\".format(content)\n",
    "\n",
    "for chunk in chatbot.chat(prompt):\n",
    "    if chunk:\n",
    "        print(chunk[\"token\"], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 interesting questions that can be used for a debate setup on the topic of the Transformer model:\n",
      "\n",
      "1. **Is the Transformer model a significant improvement over traditional recurrent neural networks (RNNs) for sequence-to-sequence tasks?**\n",
      "\t* Argument for: The Transformer model has achieved state-of-the-art results in many sequence-to-sequence tasks, such as machine translation and text summarization.\n",
      "\t* Argument against: RNNs have been widely used and have achieved good results in many tasks, and the Transformer model may not be suitable for all types of sequence-to-sequence tasks.\n",
      "2. **Does the self-attention mechanism in the Transformer model truly capture the complexities of human language?**\n",
      "\t* Argument for: The self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance, which is similar to how humans process language.\n",
      "\t* Argument against: The self-attention mechanism is a simplification of the complex processes involved in human language understanding, and may not capture all the nuances of human language.\n",
      "3. **Is the use of positional encoding in the Transformer model a necessary evil, or is there a better way to incorporate positional information into the model?**\n",
      "\t* Argument for: Positional encoding is necessary to preserve the order of the input sequence, which is important for many sequence-to-sequence tasks.\n",
      "\t* Argument against: Positional encoding can be seen as a hack, and there may be better ways to incorporate positional information into the model, such as using recurrent neural networks or other architectures.\n",
      "4. **Can the Transformer model be used for tasks beyond sequence-to-sequence, such as question answering or text classification?**\n",
      "\t* Argument for: The Transformer model has been shown to be effective in a variety of tasks beyond sequence-to-sequence, such as question answering and text classification.\n",
      "\t* Argument against: The Transformer model is specifically designed for sequence-to-sequence tasks, and may not be the best choice for other types of tasks.\n",
      "5. **Is the Transformer model too computationally expensive for practical use in many applications?**\n",
      "\t* Argument for: The Transformer model requires a significant amount of computational resources, which can be a limitation in many applications.\n",
      "\t* Argument against: The Transformer model can be optimized and parallelized to reduce computational costs, and the benefits of the model may outweigh the costs in many applications.\n",
      "6. **Does the Transformer model have any inherent biases or limitations that could affect its performance in certain tasks or domains?**\n",
      "\t* Argument for: The Transformer model, like any other machine learning model, can have biases and limitations that affect its performance in certain tasks or domains.\n",
      "\t* Argument against: The Transformer model is a general-purpose model that can be fine-tuned for specific tasks and domains, and any biases or limitations can be mitigated with proper training and evaluation.\n",
      "7. **Can the Transformer model be used for low-resource languages or domains, where there is limited training data available?**\n",
      "\t* Argument for: The Transformer model can be used for low-resource languages or domains, and has been shown to be effective in such cases.\n",
      "\t* Argument against: The Transformer model requires a significant amount of training data to achieve good results, and may not be suitable for low-resource languages or domains.\n",
      "8. **Is the Transformer model a step towards true artificial intelligence, or is it just a specialized tool for sequence-to-sequence tasks?**\n",
      "\t* Argument for: The Transformer model represents a significant step towards true artificial intelligence, as it is able to learn complex patterns and relationships in data.\n",
      "\t* Argument against: The Transformer model is a specialized tool for sequence-to-sequence tasks, and is not a general-purpose artificial intelligence model.\n",
      "9. **Can the Transformer model be used for multimodal tasks, such as speech recognition or image captioning?**\n",
      "\t* Argument for: The Transformer model can be used for multimodal tasks, and has been shown to be effective in such cases.\n",
      "\t* Argument against: The Transformer model is specifically designed for sequence-to-sequence tasks, and may not be the best choice for multimodal tasks.\n",
      "10. **Will the Transformer model replace traditional machine learning models, such as recurrent neural networks and convolutional neural networks, in the near future?**\n",
      "\t* Argument for: The Transformer model has achieved state-of-the-art results in many tasks, and may replace traditional machine learning models in the near future.\n",
      "\t* Argument against: Traditional machine learning models have been widely used and have achieved good results in many tasks, and the Transformer model may not replace them entirely in the near future.\n",
      "\n",
      "These questions can be used to stimulate a debate on the strengths and limitations of the Transformer model, and its potential applications and implications for the field of natural language processing."
     ]
    }
   ],
   "source": [
    "prompt = \"Read the following paper, don't summarize, set up 10 interesting questions with for and against premises for a debate.\\n{}\\n Don't forget to share 10 interesting questions for the debate setup\".format(content)\n",
    "\n",
    "for chunk in chatbot.chat(prompt):\n",
    "    if chunk:\n",
    "        print(chunk[\"token\"], end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-rag-o4xYDdTl-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
