{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from hugchat import hugchat\n",
    "from hugchat.login import Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv(\"./secrets.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = os.environ['HUGGING_CHAT_USERNAME']\n",
    "PASSWD = os.environ['HUGGING_CHAT_PASSWORD']\n",
    "cookie_path_dir = \"./cookies/\" # NOTE: trailing slash (/) is required to avoid errors\n",
    "sign = Login(EMAIL, PASSWD)\n",
    "cookies = sign.login(cookie_dir_path=cookie_path_dir, save_cookies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = hugchat.ChatBot(cookies=cookies.get_dict())\n",
    "\n",
    "# print(chatbot.chat(\"Tell me about Alpha Centauri\").wait_until_done())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta-llama/Llama-3.3-70B-Instruct',\n",
       " 'Qwen/Qwen2.5-72B-Instruct',\n",
       " 'CohereForAI/c4ai-command-r-plus-08-2024',\n",
       " 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B',\n",
       " 'nvidia/Llama-3.1-Nemotron-70B-Instruct-HF',\n",
       " 'Qwen/QwQ-32B-Preview',\n",
       " 'Qwen/Qwen2.5-Coder-32B-Instruct',\n",
       " 'meta-llama/Llama-3.2-11B-Vision-Instruct',\n",
       " 'NousResearch/Hermes-3-Llama-3.1-8B',\n",
       " 'mistralai/Mistral-Nemo-Instruct-2407',\n",
       " 'microsoft/Phi-3.5-mini-instruct']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = chatbot.get_available_llm_models()\n",
    "display([m.name for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_model_name = 'NousResearch/Hermes-3-Llama-3.1-8B'\n",
    "model_names = [m.name for m in models]\n",
    "idx = model_names.index(chosen_model_name)\n",
    "\n",
    "chatbot.switch_llm(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ''\n",
    "with open(os.path.join('../data', 'attention-is-all-you-need.txt'), 'r') as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 2 interesting questions for a debate setup with for and against positions related to the Transformer model:\n",
      "\n",
      "**Question 1: Should the Transformer model be used as the primary architecture for all natural language processing tasks?**\n",
      "\n",
      "* **For position:** The Transformer model has shown state-of-the-art results in many NLP tasks, including machine translation, text classification, and question answering. Its ability to handle long-range dependencies and parallelize computation makes it an ideal choice for many applications. Furthermore, the Transformer's modular design allows for easy modification and extension, making it a versatile architecture for a wide range of NLP tasks.\n",
      "* **Against position:** While the Transformer model has achieved impressive results in some NLP tasks, it may not be the best choice for all tasks. For example, tasks that require strong sequential dependencies, such as language modeling or text generation, may be better suited for recurrent neural network (RNN) architectures. Additionally, the Transformer's reliance on self-attention mechanisms can lead to computational inefficiencies and overfitting in some cases.\n",
      "\n",
      "**Question 2: Is the Transformer model's reliance on self-attention mechanisms a significant limitation for its application in real-world NLP tasks?**\n",
      "\n",
      "* **For position:** The Transformer model's self-attention mechanisms are a significant limitation for its application in real-world NLP tasks. Self-attention requires computing attention weights for all pairs of input elements, which can lead to quadratic computational complexity and make it difficult to apply the model to long sequences or large datasets. Furthermore, self-attention may not be able to capture certain types of dependencies, such as hierarchical or compositional relationships, which are important in many NLP tasks.\n",
      "* **Against position:** The Transformer model's self-attention mechanisms are a key strength, allowing the model to capture complex dependencies and relationships between input elements. While it is true that self-attention can be computationally expensive, there are many techniques that can be used to mitigate this cost, such as using sparse attention patterns or approximating attention weights. Additionally, the Transformer's self-attention mechanisms have been shown to be highly effective in many NLP tasks, including machine translation, question answering, and text classification, and are likely to remain a key component of many NLP architectures in the future."
     ]
    }
   ],
   "source": [
    "prompt = \"Read the following paper, don't summarize, set up 2 interesting questions with for and against premises for a debate.\\n{}\\n Don't forget to share 2 interesting questions for the debate setup with for and against positions\".format(content)\n",
    "\n",
    "for chunk in chatbot.chat(prompt):\n",
    "    if chunk:\n",
    "        print(chunk[\"token\"], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate_text = \"\"\"\n",
    "Here are 2 interesting questions for a debate setup with for and against positions related to the Transformer model:\n",
    "\n",
    "**Question 1: Should the Transformer model be used as the primary architecture for all natural language processing tasks?**\n",
    "\n",
    "* **For position:** The Transformer model has shown state-of-the-art results in many NLP tasks, including machine translation, text classification, and question answering. Its ability to handle long-range dependencies and parallelize computation makes it an ideal choice for many applications. Furthermore, the Transformer's modular design allows for easy modification and extension, making it a versatile architecture for a wide range of NLP tasks.\n",
    "* **Against position:** While the Transformer model has achieved impressive results in some NLP tasks, it may not be the best choice for all tasks. For example, tasks that require strong sequential dependencies, such as language modeling or text generation, may be better suited for recurrent neural network (RNN) architectures. Additionally, the Transformer's reliance on self-attention mechanisms can lead to computational inefficiencies and overfitting in some cases.\n",
    "\n",
    "**Question 2: Is the Transformer model's reliance on self-attention mechanisms a significant limitation for its application in real-world NLP tasks?**\n",
    "\n",
    "* **For position:** The Transformer model's self-attention mechanisms are a significant limitation for its application in real-world NLP tasks. Self-attention requires computing attention weights for all pairs of input elements, which can lead to quadratic computational complexity and make it difficult to apply the model to long sequences or large datasets. Furthermore, self-attention may not be able to capture certain types of dependencies, such as hierarchical or compositional relationships, which are important in many NLP tasks.\n",
    "* **Against position:** The Transformer model's self-attention mechanisms are a key strength, allowing the model to capture complex dependencies and relationships between input elements. While it is true that self-attention can be computationally expensive, there are many techniques that can be used to mitigate this cost, such as using sparse attention patterns or approximating attention weights. Additionally, the Transformer's self-attention mechanisms have been shown to be highly effective in many NLP tasks, including machine translation, question answering, and text classification, and are likely to remain a key component of many NLP architectures in the future.\n",
    "\"\"\"\n",
    "\n",
    "def extract_topics(text):\n",
    "    debate = {'1': {}, '2': {}}\n",
    "    lines = text.split(\"\\n\")\n",
    "    curr = 1\n",
    "    for_pattern1 = '**For position:**'.lower()\n",
    "    for_pattern2 = '**For:**'.lower()\n",
    "    against_pattern1 = '**Against position:**'.lower()\n",
    "    against_pattern2 = '**Against:**'.lower()\n",
    "    ques_pattern = '**Question 1'.lower()\n",
    "    for l in lines:\n",
    "        if len(debate[str(curr)].keys()) == 3:\n",
    "            curr += 1\n",
    "        if ques_pattern in l.lower():\n",
    "            idx = l.lower().find(ques_pattern)\n",
    "            debate[str(curr)]['question'] = l[idx + len(ques_pattern) + 1:]\n",
    "        if for_pattern1 in l.lower():\n",
    "            idx = l.lower().find(for_pattern1)\n",
    "            if curr == 1:\n",
    "                debate[str(curr)]['for'] = l[idx + len(for_pattern1) + 1:]\n",
    "            else:\n",
    "                debate[str(curr)]['for'] = l[idx + len(for_pattern1) + 1:]\n",
    "        elif for_pattern2 in l.lower():\n",
    "            idx = l.lower().find(for_pattern1)\n",
    "            if curr == 1:\n",
    "                debate[str(curr)]['for'] = l[idx + len(for_pattern1) + 1:]\n",
    "            else:\n",
    "                debate[str(curr)]['for'] = l[idx + len(for_pattern1) + 1:]\n",
    "        if against_pattern1 in l.lower():\n",
    "            idx = l.lower().find(against_pattern1)\n",
    "            if curr == 1:\n",
    "                debate[str(curr)]['against'] = l[idx + len(against_pattern1) + 1:]\n",
    "            else:\n",
    "                debate[str(curr)]['against'] = l[idx + len(against_pattern1) + 1:]\n",
    "        elif against_pattern2 in l.lower():\n",
    "            idx = l.lower().find(against_pattern2)\n",
    "            if curr == 1:\n",
    "                debate[str(curr)]['against'] = l[idx + len(against_pattern2) + 1:]\n",
    "            else:\n",
    "                debate[str(curr)]['against'] = l[idx + len(against_pattern2) + 1:]\n",
    "    return debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'question': ' Should the Transformer model be used as the primary architecture for all natural language processing tasks?**',\n",
       "  'for': \"The Transformer model has shown state-of-the-art results in many NLP tasks, including machine translation, text classification, and question answering. Its ability to handle long-range dependencies and parallelize computation makes it an ideal choice for many applications. Furthermore, the Transformer's modular design allows for easy modification and extension, making it a versatile architecture for a wide range of NLP tasks.\",\n",
       "  'against': \"While the Transformer model has achieved impressive results in some NLP tasks, it may not be the best choice for all tasks. For example, tasks that require strong sequential dependencies, such as language modeling or text generation, may be better suited for recurrent neural network (RNN) architectures. Additionally, the Transformer's reliance on self-attention mechanisms can lead to computational inefficiencies and overfitting in some cases.\"},\n",
       " '2': {'for': \"The Transformer model's self-attention mechanisms are a significant limitation for its application in real-world NLP tasks. Self-attention requires computing attention weights for all pairs of input elements, which can lead to quadratic computational complexity and make it difficult to apply the model to long sequences or large datasets. Furthermore, self-attention may not be able to capture certain types of dependencies, such as hierarchical or compositional relationships, which are important in many NLP tasks.\",\n",
       "  'against': \"The Transformer model's self-attention mechanisms are a key strength, allowing the model to capture complex dependencies and relationships between input elements. While it is true that self-attention can be computationally expensive, there are many techniques that can be used to mitigate this cost, such as using sparse attention patterns or approximating attention weights. Additionally, the Transformer's self-attention mechanisms have been shown to be highly effective in many NLP tasks, including machine translation, question answering, and text classification, and are likely to remain a key component of many NLP architectures in the future.\"}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_topics(debate_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-rag-o4xYDdTl-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
