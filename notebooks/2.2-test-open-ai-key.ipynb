{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv(\"./secrets.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\")\n",
    "# llm = OpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Centauri is the closest star system to Earth, located about 4.37 light-years away. It consists of three stars: Alpha Centauri A and B, which are sun-like stars, and Proxima Centauri, a red dwarf. Proxima Centauri is the closest of the three and hosts at least one confirmed exoplanet, Proxima Centauri b, situated in its habitable zone. The system has been a target for exploration and study due to its proximity and potential for hosting life-supporting conditions."
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about Alpha Centauri in less than 100 words\"\n",
    "for chunk in chat.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ''\n",
    "with open(os.path.join('../data', 'attention-is-all-you-need.txt'), 'r') as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 interesting questions with for and against premises for a debate based on the ideas presented in the paper \"Attention Is All You Need\":\n",
      "\n",
      "1. **Question 1:** Is the Transformer architecture superior to recurrent neural networks (RNNs) for sequence transduction tasks?\n",
      "   - **For:** The Transformer achieves better performance while enabling more parallelization, making it more efficient for training on large datasets.\n",
      "   - **Against:** RNNs have been extensively fine-tuned and are effective for tasks with complex temporal dependencies, which the Transformer may not readily capture.\n",
      "\n",
      "2. **Question 2:** Does the reliance on self-attention diminish the model's ability to learn long-range dependencies in sequences?\n",
      "   - **For:** Self-attention mechanisms connect all positions in a sequence, allowing for better handling of long-range dependencies without the sequential constraints of RNNs.\n",
      "   - **Against:** Self-attention may introduce ambiguity and lead to information dilution, particularly in very long sequences.\n",
      "\n",
      "3. **Question 3:** Should the use of multi-head attention be considered a necessary advance in model architecture?\n",
      "   - **For:** Multi-head attention allows the model to focus on different representation subspaces and diverse aspects of the input, improving overall performance.\n",
      "   - **Against:** The complexity introduced by multi-head attention adds computational overhead that may not be justified for simpler tasks.\n",
      "\n",
      "4. **Question 4:** Is the computational efficiency of the Transformer significant enough to warrant its adoption as the standard for neural network architectures?\n",
      "   - **For:** The parallelizability and reduced training time associated with the Transformer make it an attractive option for real-world applications.\n",
      "   - **Against:** The efficiency gains may not be substantial enough to outweigh the increased complexity in model design and tuning.\n",
      "\n",
      "5. **Question 5:** Do positional encodings enhance the performance of Transformers in a meaningful way?\n",
      "   - **For:** Positional encodings are essential for preserving the order of tokens in sequences, which is critical for language tasks.\n",
      "   - **Against:** The choice of positional encoding method may have negligible impact compared to advancements in model architecture and learning strategies.\n",
      "\n",
      "6. **Question 6:** Can the principles of the Transformer be effectively applied to domains beyond natural language processing, such as image or audio data?\n",
      "   - **For:** The attention mechanism demonstrates promise across modalities, suggesting versatility and generalizability of the Transformer architecture.\n",
      "   - **Against:** Specific characteristics and structures of non-text data may require fundamentally different approaches than those offered by the Transformer.\n",
      "\n",
      "7. **Question 7:** Is it beneficial to use attention visualizations to interpret the model's decision-making processes?\n",
      "   - **For:** Visualizations can provide insights into what the model attends to, enhancing our understanding and trust in AI systems.\n",
      "   - **Against:** Attention visualizations can be misleading and may not truly represent the underlying mechanisms of the model.\n",
      "\n",
      "8. **Question 8:** Should future research prioritize refining the Transformer architecture over exploring other potential architectures?\n",
      "   - **For:** The Transformer architecture shows significant promise and has achieved state-of-the-art performance, warranting further exploration and optimization.\n",
      "   - **Against:** Innovation in AI could stall if researchers overly focus on one architecture; exploration of diverse methodologies may yield unforeseen advantages.\n",
      "\n",
      "9. **Question 9:** Are the advantages of the Transformer model in performance and efficiency worth the architectural complexity it introduces?\n",
      "   - **For:** The state-of-the-art performance achieved by Transformers validates the complexity as a beneficial trade-off in many high-stakes applications.\n",
      "   - **Against:** The increased complexity could hinder model accessibility and usability, especially for practitioners with limited resources.\n",
      "\n",
      "10. **Question 10:** Will Transformers eventually replace all previous models in sequence processing tasks?\n",
      "    - **For:** The capabilities and efficiencies provided by Transformers suggest a trend toward their dominance in various sequence processing applications.\n",
      "    - **Against:** Specific tasks may still be better suited for traditional models like RNNs or CNNs due to their unique strengths, ensuring that a variety of models will coexist."
     ]
    }
   ],
   "source": [
    "prompt = \"Read the following paper, don't summarize, set up 10 interesting questions with for and against premises for a debate.\\n{}\\n Don't forget to share 10 interesting questions for the debate setup\".format(content)\n",
    "\n",
    "for chunk in chat.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-rag-o4xYDdTl-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
